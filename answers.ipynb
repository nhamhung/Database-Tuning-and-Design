{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations, permutations, product\n",
    "\n",
    "## Q1a. Determine the closure of a given set of attribute S the schema R and functional dependency F\n",
    "def closure(R, F, S):\n",
    "    \"\"\"\n",
    "    Compute closure of an att_set based on the fd_set\n",
    "    \"\"\"\n",
    "    closed = set(S)\n",
    "    unused = F.copy()\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        for fd in unused.copy():\n",
    "            # lhs of current fd is a subset of att_set closed\n",
    "            if set(fd[0]).issubset(closed):\n",
    "                closed.update(set(fd[1]))\n",
    "                # early termination\n",
    "                if len(closed) == len(R):\n",
    "                    break\n",
    "                unused.remove(fd)\n",
    "                changed = True\n",
    "            \n",
    "    return list(closed)\n",
    "\n",
    "## Q1b. Determine all attribute closures excluding superkeys that are not candidate keys given the schema R and functional dependency F\n",
    "def all_closures(R, F): \n",
    "    \"\"\"\n",
    "    Compute closures of all subsets of the fd_set excluding super keys that are not candidate keys\n",
    "    \"\"\"\n",
    "    all_subsets = get_all_subsets(R)\n",
    "    result = []\n",
    "\n",
    "    all_subsets_copy = all_subsets.copy()\n",
    "\n",
    "    while len(all_subsets_copy) > 0:\n",
    "        attr_set = all_subsets_copy.pop(0)\n",
    "        attr_set_closure = closure(R, F, attr_set)\n",
    "        if len(attr_set_closure) == 0:\n",
    "            continue\n",
    "        if set(attr_set_closure) == set(R):\n",
    "            all_subsets_copy = [attr_set_copy for attr_set_copy in all_subsets_copy if not set(attr_set_copy).issuperset(set(attr_set))]\n",
    "    \n",
    "        result.append([list(attr_set), attr_set_closure])\n",
    "\n",
    "    return result\n",
    "    \n",
    "## Q2a. Return a minimal cover of the functional dependencies of a given schema R and functional dependencies F.\n",
    "def min_cover(R, FD): \n",
    "    \"\"\"\n",
    "    Compute a minimal cover of the given fd_set according to the following steps:\n",
    "    1. Minimize rhs of all FDs\n",
    "    2. Minimize lhs of all FDs\n",
    "    3. Remove redundant FDs\n",
    "    \"\"\"\n",
    "    result = minimize_rhs_all_fds(FD)\n",
    "    result = minimize_lhs_all_fds(R, result)\n",
    "    minimal_cover = remove_redundant_fds(R, result)\n",
    "    return minimal_cover\n",
    "\n",
    "## Q2b. Return all minimal covers reachable from the functional dependencies of a given schema R and functional dependencies F.\n",
    "def min_covers(R, FD):\n",
    "    \"\"\"\n",
    "    Compute all minimal covers reachable from the given FD according to the following steps:\n",
    "    1. Minimize rhs of all FDs\n",
    "    2. Get all possible combinations of left-minimized FDs\n",
    "    3. For each combination which is a left-minimized FD, get unique minimal covers from all of its permutations\n",
    "    \"\"\"\n",
    "    # minimize rhs\n",
    "    rminimized_fd_set = minimize_rhs_all_fds(FD)\n",
    "    all_lminimized = {}\n",
    "    # compute all possible minimized subsets of each lhs\n",
    "    for lhs, rhs in rminimized_fd_set:\n",
    "        all_lminimized[(tuple(lhs), tuple(rhs))] = set()\n",
    "\n",
    "        min_len = len(lhs) \n",
    "        for lhs_subset in chain.from_iterable(combinations(lhs, r) for r in range(0, len(lhs) + 1)):\n",
    "            if implies(R, list(lhs_subset), rhs, rminimized_fd_set) and len(lhs_subset) <= min_len:\n",
    "                min_len = len(lhs_subset)\n",
    "                all_lminimized[(tuple(lhs), tuple(rhs))].add(tuple(sorted(list(lhs_subset))))\n",
    "    \n",
    "    x = [list(lhs) for lhs in list(all_lminimized.values())]\n",
    "    all_rhs = [rhs for _, rhs in rminimized_fd_set]\n",
    "    # use cartesian product to get all possible left-minimized fd_set\n",
    "    cartesian_product = [[list(x) for x in group] for group in list(product(*x))]\n",
    "    all_lminimized_fd_set = [list(zip(product, all_rhs)) for product in cartesian_product]\n",
    "    all_lminimized_fd_set = [[list(x) for x in group] for group in all_lminimized_fd_set]\n",
    "\n",
    "    # for each left-minimized fd_set, get all unique minimal covers for different permutations\n",
    "    all_minimal_covers = set()\n",
    "\n",
    "    for lminimized_fd_set in all_lminimized_fd_set:\n",
    "        for permutation in permutations(lminimized_fd_set, len(lminimized_fd_set)):\n",
    "            minimum_cover = remove_redundant_fds(R, list(permutation))\n",
    "            hashable_minimum_cover = get_hashable_fd_set(minimum_cover)\n",
    "            all_minimal_covers.add(hashable_minimum_cover)\n",
    "\n",
    "    return [[[list(lhs), list(rhs)] for lhs, rhs in min_cover] for min_cover in all_minimal_covers]\n",
    "\n",
    "## Q2c. Return all minimal covers of a given schema R and functional dependencies F.\n",
    "def all_min_covers(R, FD):\n",
    "    \"\"\"\n",
    "    Get all minimal covers for a given FD according to the following steps:\n",
    "    1. Compute FD+ from FD\n",
    "    2. Remove trivial fd from FD+\n",
    "    3. Apply min_covers on FD+ to obtain all minimal covers\n",
    "    \"\"\"\n",
    "    all_minimal_covers = set()\n",
    "    # compute fd_plus which is the set closure of fd_set\n",
    "    fd_plus = set_closure(R, FD)\n",
    "    fd_plus = [fd for fd in fd_plus if not set(fd[0]).issuperset(set(fd[1]))]\n",
    "    # apply min_covers on fd_plus to get all minimal covers\n",
    "    minimal_covers = min_covers(R, fd_plus)\n",
    "\n",
    "    for minimum_cover in minimal_covers:\n",
    "        all_minimal_covers.add(get_hashable_fd_set(minimum_cover))\n",
    "\n",
    "    return [[[list(lhs), list(rhs)] for lhs, rhs in min_cover] for min_cover in all_minimal_covers]\n",
    "\n",
    "## You can add additional functions below\n",
    "\n",
    "def get_all_subsets(lst):\n",
    "    return list(chain.from_iterable(combinations(lst, r) for r in range(0, len(lst) + 1)))\n",
    "\n",
    "def implies(schema, lhs, rhs, fd_set):\n",
    "    \"\"\"\n",
    "    Check whether the current fd_set implies the fd [lhs, rhs]\n",
    "    \"\"\"\n",
    "    return set(closure(schema, fd_set, lhs)).issuperset(set(rhs))\n",
    "\n",
    "def set_closure(schema, fd_set):\n",
    "    \"\"\"\n",
    "    Get the set closure of a given fd_set\n",
    "    \"\"\"\n",
    "    fd_plus = []\n",
    "    L = get_all_subsets(schema)\n",
    "    \n",
    "    for attr_set in L:\n",
    "        attr_set_closure = closure(schema, fd_set, attr_set)\n",
    "        for attr in attr_set_closure:\n",
    "            fd_plus.append([list(attr_set), list(attr)])\n",
    "        \n",
    "    return fd_plus\n",
    "\n",
    "def get_hashable_fd_set(fd_set):\n",
    "    \"\"\"\n",
    "    Get a hashable version of the fd_set to remove duplicates\n",
    "    \"\"\"\n",
    "    return tuple(sorted(list(((tuple(lhs), tuple(rhs)) for lhs, rhs in fd_set))))\n",
    "\n",
    "def minimize_lhs_current_fd(schema, lhs, rhs, fd_set):\n",
    "    \"\"\"\n",
    "    Minimize the lhs of this fd [lhs, rhs] by removing each attribute of lhs\n",
    "    \"\"\"\n",
    "    lhs_copy = lhs.copy()\n",
    "    for attr in lhs:\n",
    "        lhs_copy.remove(attr)\n",
    "        if not implies(schema, lhs_copy, rhs, fd_set):\n",
    "            lhs_copy.append(attr)\n",
    "    return lhs_copy\n",
    "\n",
    "def minimize_rhs_all_fds(fd_set):\n",
    "    \"\"\"\n",
    "    Minimize the rhs to a single attribute for every fd in the fd_set\n",
    "    \"\"\"\n",
    "    fd_set_rminimized = []\n",
    "\n",
    "    for lhs, rhs in fd_set:\n",
    "        for attr in rhs:\n",
    "            if [lhs, attr] not in fd_set_rminimized:\n",
    "                fd_set_rminimized.append([lhs, [attr]])\n",
    "    \n",
    "    return fd_set_rminimized\n",
    "\n",
    "def minimize_lhs_all_fds(schema, fd_set):\n",
    "    \"\"\"\n",
    "    Minimize the lhs for every fd in the fd_set\n",
    "    \"\"\"\n",
    "    fd_set_lminimized = []\n",
    "    for lhs, rhs in fd_set:\n",
    "        lhs_minimized = minimize_lhs_current_fd(schema, lhs, rhs, fd_set)\n",
    "        if lhs_minimized != lhs:\n",
    "            fd_set_lminimized.append([lhs_minimized, rhs])\n",
    "        else:\n",
    "            fd_set_lminimized.append([lhs, rhs])\n",
    "    return fd_set_lminimized\n",
    "\n",
    "def remove_redundant_fds(schema, fd_set):\n",
    "    \"\"\"\n",
    "    Check whether without each FD fd, the fd_set can still imply fd\n",
    "    \"\"\"\n",
    "    fd_set_copy = fd_set.copy()\n",
    "    for fd in fd_set:\n",
    "        fd_set_copy.remove(fd)\n",
    "        if not implies(schema, fd[0], fd[1], fd_set_copy):\n",
    "            fd_set_copy.append(fd)\n",
    "    \n",
    "    return fd_set_copy\n",
    "\n",
    "def find_all_candidate_keys(R, F):\n",
    "    all_subsets = get_all_subsets(R)\n",
    "    result = []\n",
    "\n",
    "    all_subsets_copy = all_subsets.copy()\n",
    "\n",
    "    while len(all_subsets_copy) > 0:\n",
    "        attr_set = all_subsets_copy.pop(0)\n",
    "        attr_set_closure = closure(R, F, list(attr_set))\n",
    "        if len(attr_set_closure) == 0:\n",
    "            continue\n",
    "        if set(attr_set_closure) == set(R):\n",
    "            all_subsets_copy = [attr_set_copy for attr_set_copy in all_subsets_copy if not set(attr_set_copy).issuperset(set(attr_set))]\n",
    "            result.append(list(attr_set))\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_prime_attributes(candidate_keys):\n",
    "    return list(set([k for ckey in candidate_keys for k in ckey]))\n",
    "\n",
    "def is_proper_subset(set1, set2):\n",
    "    return set1.issubset(set2) and not set2.issubset(set1)\n",
    "\n",
    "def is_3NF(R, F):\n",
    "    minimal_cover = min_cover(R, F)\n",
    "    print(\"Minimal cover: \", minimal_cover)\n",
    "    candidate_keys = find_all_candidate_keys(R, F)\n",
    "    print(\"Candidate keys: \", candidate_keys)\n",
    "    prime_attributes = get_prime_attributes(candidate_keys)\n",
    "    print(\"Prime attributes: \", prime_attributes, end=\"\\n\")\n",
    "    violating_fds = []\n",
    "    is_3nf = True\n",
    "    for fd in minimal_cover:\n",
    "        lhs = fd[0]\n",
    "        rhs = fd[1][0]\n",
    "        for ckey in candidate_keys:\n",
    "            if not set(lhs).issuperset(set(ckey)):\n",
    "                if rhs not in prime_attributes:\n",
    "                    print(\"-\"*30)\n",
    "                    print(\"Current fd: \", fd)\n",
    "                    print(\"Current candidate key: \", ckey)\n",
    "                    print(f\"{lhs} is not a super key\")\n",
    "                    print(f\"{rhs} is not a prime attribute\")\n",
    "                    if fd not in violating_fds:\n",
    "                        violating_fds.append(fd)\n",
    "                    is_3nf = False\n",
    "    \n",
    "    return is_3nf, violating_fds\n",
    "\n",
    "def compact_min_cover(minimal_cover):\n",
    "    lhs_map = {}\n",
    "    for fd in minimal_cover:\n",
    "        lhs, rhs = fd[0], fd[1]\n",
    "        hashable_lhs = tuple(sorted(lhs))\n",
    "        if hashable_lhs not in lhs_map:\n",
    "            lhs_map[hashable_lhs] = set(rhs)\n",
    "        else:\n",
    "            lhs_map[hashable_lhs].update(set(rhs))\n",
    "    \n",
    "    return [[list(lhs), list(rhs)] for lhs, rhs in lhs_map.items()]\n",
    "    \n",
    "def is_BCNF(R, F, verbose=True):\n",
    "    minimal_cover = min_cover(R, F)\n",
    "    minimal_cover = compact_min_cover(minimal_cover)\n",
    "    candidate_keys = find_all_candidate_keys(R, F)\n",
    "    if verbose:\n",
    "        print(f\"Checking Relation {R} with FD set {F}\")\n",
    "        print(\"Minimal cover: \", minimal_cover)\n",
    "        print(\"Candidate keys: \", candidate_keys)\n",
    "    is_bcnf = True\n",
    "    violating_fds = []\n",
    "    # Check if any fd's lhs is not a superkey\n",
    "    for fd in minimal_cover:\n",
    "        lhs = fd[0]\n",
    "        # Check if current fd's lhs is a superkey by checking if it is a subset of any candidate_keys\n",
    "        is_lhs_superkey = False\n",
    "        for ckey in candidate_keys:\n",
    "            if set(lhs).issuperset(set(ckey)):\n",
    "                is_lhs_superkey = True\n",
    "        # If not a superkey, then BCNF is violated. Add fd to violating_fds if not exists\n",
    "        if not is_lhs_superkey:\n",
    "            if fd not in violating_fds:\n",
    "                violating_fds.append(fd)\n",
    "            is_bcnf = False\n",
    "    \n",
    "    return is_bcnf, violating_fds\n",
    "\n",
    "def is_2NF(R, F):\n",
    "    minimal_cover = min_cover(R, F)\n",
    "    print(\"Minimal cover: \", minimal_cover)\n",
    "    candidate_keys = find_all_candidate_keys(R, F)\n",
    "    print(\"Candidate keys: \", candidate_keys)\n",
    "    prime_attributes = get_prime_attributes(candidate_keys)\n",
    "    print(\"Prime attributes: \", prime_attributes, end=\"\\n\")\n",
    "    violating_fds = []\n",
    "    is_2nf = True\n",
    "    for fd in minimal_cover:\n",
    "        lhs = fd[0]\n",
    "        rhs = fd[1][0]\n",
    "        for ckey in candidate_keys:\n",
    "            if is_proper_subset(set(lhs), set(ckey)):\n",
    "                if rhs not in prime_attributes:\n",
    "                    print(\"-\"*30)\n",
    "                    print(\"Current fd: \", fd)\n",
    "                    print(f\"{lhs} is a proper subset of candidate key {ckey}\")\n",
    "                    print(f\"{rhs} is not a prime attribute\")\n",
    "                    if fd not in violating_fds:\n",
    "                        violating_fds.append(fd)\n",
    "                    is_2nf = False\n",
    "    \n",
    "    return is_2nf, violating_fds\n",
    "\n",
    "def project(parent_R, decomposed_R, parent_F):\n",
    "    decomposed_F = []\n",
    "    for attr_set in chain.from_iterable(combinations(decomposed_R, r) for r in range(1, len(decomposed_R) + 1)):\n",
    "        attr_set_closure = closure(decomposed_R, parent_F, list(attr_set))\n",
    "\n",
    "        for attr in attr_set_closure:\n",
    "            if attr in decomposed_R:\n",
    "                decomposed_F.append([list(attr_set), list(attr)])\n",
    "    \n",
    "    minimal_cover = min_cover(decomposed_R, decomposed_F)\n",
    "    return minimal_cover\n",
    "\n",
    "def decomposision(R, F, accum):\n",
    "    is_bcnf, violating_fds = is_BCNF(R, F, verbose=True)\n",
    "    if not is_bcnf:\n",
    "        violating_fd = violating_fds[0]\n",
    "        lhs = violating_fd[0]\n",
    "        R1 = closure(R, F, lhs)\n",
    "        R2 = list(set(R).difference(set(R1)).union(set(lhs)))\n",
    "        F1 = project(R, R1, F)\n",
    "        F2 = project(R, R2, F)\n",
    "        print(f\"Violating Relation: {R}\")\n",
    "        print(f\"Violating fds: {violating_fds}\")\n",
    "        print(f\"R1: {R1} - F1: {F1}\")\n",
    "        print(f\"R2: {R2} - F2: {F2}\")\n",
    "        print(\"-\" * 30)\n",
    "        decomposision(R1, F1, accum)\n",
    "        decomposision(R2, F2, accum)\n",
    "    else:\n",
    "        accum.append((R, F))\n",
    "    \n",
    "    return accum\n",
    "\n",
    "def remove_subsumed_fragments(lst):\n",
    "    curr_res = []\n",
    "    result = []\n",
    "    # map each element to set and sort by decreasing size\n",
    "    for fragment in sorted(map(set, lst), key = len, reverse = True):\n",
    "        # if current fragment is not a subset of any fragment in curr_res, add it to curr_res. This helps to remove duplicate fragment of same size also as the next occurrence will be a subset hence not added.\n",
    "        if not any(fragment.issubset(req) for req in curr_res):\n",
    "            curr_res.append(fragment)\n",
    "            result.append(list(fragment))\n",
    "          \n",
    "    return result\n",
    "\n",
    "def make_fragments(compact_minimal_cover, candidate_keys):\n",
    "    fragments = [lhs + rhs for lhs, rhs in compact_minimal_cover]\n",
    "    print(f\"Original Fragments {fragments}\")\n",
    "    # remove subsumed fragments (any fragment which is a proper subset of any other fragment)\n",
    "    no_subsumed_fragments = remove_subsumed_fragments(fragments)\n",
    "    print(f\"After remove subsumed fragments {no_subsumed_fragments}\")\n",
    "    \n",
    "    # check if any fragments contains a candidate key\n",
    "    has_ckey = False\n",
    "    for fragment in no_subsumed_fragments:\n",
    "        for ckey in candidate_keys:\n",
    "            if set(ckey).issubset(set(fragment)):\n",
    "                has_ckey = True\n",
    "    print(f\"Current fragments have candidate key? {has_ckey}\")\n",
    "    if not has_ckey:\n",
    "        no_subsumed_fragments.append(candidate_keys[0])\n",
    "    print(f\"Append {candidate_keys[0]} since no fragment contains a candidate key\")\n",
    "\n",
    "    return no_subsumed_fragments\n",
    "\n",
    "def synthesis(R, F):\n",
    "    print(f\"Relation {R} with FD set {F}\")\n",
    "    candidate_keys = find_all_candidate_keys(R, F)\n",
    "    print(f\"Candidate keys {candidate_keys}\")\n",
    "    minimal_cover = min_cover(R, F)\n",
    "    compact_minimal_cover = compact_min_cover(minimal_cover)\n",
    "    print(f\"Compact min cover: {compact_minimal_cover}\")\n",
    "    fragments = make_fragments(compact_minimal_cover, candidate_keys)\n",
    "    print(f\"Fragments {fragments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Relation ['A', 'B', 'C', 'D', 'E'] with FD set [[['A'], ['A', 'B', 'C']], [['A', 'B'], ['A']], [['B', 'C'], ['A', 'D']], [['B'], ['A', 'B']], [['C'], ['D']]]\n",
      "Minimal cover:  [[['A'], ['C', 'B']], [['B'], ['A']], [['C'], ['D']]]\n",
      "Candidate keys:  [['A', 'E'], ['B', 'E']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, [[['A'], ['C', 'B']], [['B'], ['A']], [['C'], ['D']]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R = ['A', 'B', 'C', 'D', 'E']\n",
    "# F = [[['A', 'B'], ['C', 'D', 'E']], [['A', 'C'], ['B', 'D', 'E']], [['B'], ['C']], [['C'], ['B']], [['C'], ['D']], [['B'], ['E']], [['C'], ['E']]]\n",
    "\n",
    "R = ['A', 'B', 'C', 'D', 'E']\n",
    "F = [[['A'], ['A', 'B', 'C']], [['A', 'B'], ['A']], [['B', 'C'], ['A', 'D']], [['B'], ['A', 'B']], [['C'], ['D']]]\n",
    "\n",
    "# min_cover(R, F)\n",
    "# is_2NF(R, F)\n",
    "# is_3NF(R, F)\n",
    "is_BCNF(R, F, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Relation ['A', 'B', 'C', 'D', 'E'] with FD set [[['A'], ['A', 'B', 'C']], [['A', 'B'], ['A']], [['B', 'C'], ['A', 'D']], [['B'], ['A', 'B']], [['C'], ['D']]]\n",
      "Minimal cover:  [[['A'], ['C', 'B']], [['B'], ['A']], [['C'], ['D']]]\n",
      "Candidate keys:  [['A', 'E'], ['B', 'E']]\n",
      "Violating Relation: ['A', 'B', 'C', 'D', 'E']\n",
      "Violating fds: [[['A'], ['C', 'B']], [['B'], ['A']], [['C'], ['D']]]\n",
      "R1: ['D', 'C', 'A', 'B'] - F1: [[['C'], ['D']], [['A'], ['B']], [['B'], ['C']], [['B'], ['A']]]\n",
      "R2: ['A', 'E'] - F2: []\n",
      "------------------------------\n",
      "Checking Relation ['D', 'C', 'A', 'B'] with FD set [[['C'], ['D']], [['A'], ['B']], [['B'], ['C']], [['B'], ['A']]]\n",
      "Minimal cover:  [[['C'], ['D']], [['A'], ['B']], [['B'], ['C', 'A']]]\n",
      "Candidate keys:  [['A'], ['B']]\n",
      "Violating Relation: ['D', 'C', 'A', 'B']\n",
      "Violating fds: [[['C'], ['D']]]\n",
      "R1: ['D', 'C'] - F1: [[['C'], ['D']]]\n",
      "R2: ['C', 'B', 'A'] - F2: [[['B'], ['A']], [['A'], ['C']], [['A'], ['B']]]\n",
      "------------------------------\n",
      "Checking Relation ['D', 'C'] with FD set [[['C'], ['D']]]\n",
      "Minimal cover:  [[['C'], ['D']]]\n",
      "Candidate keys:  [['C']]\n",
      "Checking Relation ['C', 'B', 'A'] with FD set [[['B'], ['A']], [['A'], ['C']], [['A'], ['B']]]\n",
      "Minimal cover:  [[['B'], ['A']], [['A'], ['C', 'B']]]\n",
      "Candidate keys:  [['B'], ['A']]\n",
      "Checking Relation ['A', 'E'] with FD set []\n",
      "Minimal cover:  []\n",
      "Candidate keys:  [['A', 'E']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['D', 'C'], [[['C'], ['D']]]),\n",
       " (['C', 'B', 'A'], [[['B'], ['A']], [['A'], ['C']], [['A'], ['B']]]),\n",
       " (['A', 'E'], [])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decomposision(R, F, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation ['A', 'B', 'C', 'D', 'E'] with FD set [[['A'], ['A', 'B', 'C']], [['A', 'B'], ['A']], [['B', 'C'], ['A', 'D']], [['B'], ['A', 'B']], [['C'], ['D']]]\n",
      "Candidate keys [['A', 'E'], ['B', 'E']]\n",
      "Compact min cover: [[['A'], ['C', 'B']], [['B'], ['A']], [['C'], ['D']]]\n",
      "Original Fragments [['A', 'C', 'B'], ['B', 'A'], ['C', 'D']]\n",
      "After remove subsumed fragments [['C', 'B', 'A'], ['D', 'C']]\n",
      "Current fragments have candidate key? False\n",
      "Append ['A', 'E'] since no fragment contains a candidate key\n",
      "Fragments [['C', 'B', 'A'], ['D', 'C'], ['A', 'E']]\n"
     ]
    }
   ],
   "source": [
    "synthesis(R, F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d97dbaaf0f8d39e846d8bff06dfa96e0d39161eb68e322a2f5c8dc0058611f2b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('covidRL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

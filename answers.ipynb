{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Determine the closure of set of attribute S given the schema R and functional dependency F\n",
    "def closure(R, F, S):\n",
    "    # make copy of F, S so that they can be called by other functions safely\n",
    "    new_f = F.copy()\n",
    "    result = S.copy()\n",
    "    # flag to stop iteration when no more functional dependency can be drawn from F\n",
    "    to_stop = False\n",
    "    while len(new_f) > 0 and not to_stop:\n",
    "        # in each loop, initialize to_stop to True\n",
    "        to_stop = True\n",
    "        # loop through all new_f to find dependency that has its condition in result set\n",
    "        temp = new_f.copy()\n",
    "        new_f = []\n",
    "        for dependency in temp:\n",
    "            # if found, remove this dependency from new_f\n",
    "            # add the element to the result set if it's not there yet\n",
    "            # also set to_stop to Fasle because we have new item in result set\n",
    "            if is_subset(result, dependency[0]):\n",
    "                for item in dependency[1]:\n",
    "                    if item not in result:\n",
    "                        result.append(item)\n",
    "                        to_stop = False\n",
    "            # if not found, may need to check in future runs\n",
    "            else:\n",
    "                new_f.append(dependency)\n",
    "    return result\n",
    "\n",
    "from itertools import chain, combinations, permutations, product\n",
    "\n",
    "def get_all_subsets(lst):\n",
    "    return list(chain.from_iterable(combinations(lst, r) for r in range(0, len(lst) + 1)))\n",
    "\n",
    "# Determine the all the attribute closure excluding superkeys that are not candidate keys given the schema R and functional dependency F\n",
    "def all_closures(R, F):\n",
    "    # call the recursive worker function with initialized values\n",
    "    # result will be added to all_result array\n",
    "    # super_key_list is a helper list to keep all found super keys\n",
    "    all_result = []\n",
    "    super_key_list = []\n",
    "    # calculate in order of the size of a subset\n",
    "    # because once a superkey k of size s is found\n",
    "    # all subsets with size > s and is a superset of k can be discarded\n",
    "    for i in range(len(R)):\n",
    "        all_set = []\n",
    "        # generate all subset with length i\n",
    "        get_all_set_rec(R, i + 1, [], all_set, super_key_list)\n",
    "        for subset in all_set:\n",
    "            # for each subset, calculate its closure\n",
    "            closure_result = closure(R, F, subset)\n",
    "            # if it's a superkey, put it into super_key_list\n",
    "            if is_subset(closure_result, R):\n",
    "                super_key_list.append(subset)\n",
    "            # add it to the final result\n",
    "            result = [subset, closure_result]\n",
    "            all_result.append(result)\n",
    "    return all_result\n",
    "\n",
    "\n",
    "## Return a minimal cover of the functional dependencies of a given schema R and functional dependencies F.\n",
    "def min_cover(R, FD):\n",
    "    new_fd = min_cover_step1(R, FD)\n",
    "    return min_cover_step2(R, new_fd)\n",
    "\n",
    "\n",
    "def min_covers(R, FD):\n",
    "    '''\n",
    "    Call min_covers_step1 and get new_fds after\n",
    "      1. simplify RHS\n",
    "      2. Find all possible ways to simplify LHS and keep all combinations\n",
    "          eg. if ['A', 'B', 'C'] can be simplified by both ['A', 'B'] and ['B', 'C'],\n",
    "          then we need to keep both possibilities as different FD sets to be considered further\n",
    "    From each new_fd in new_fds, we try to find a min_cover by\n",
    "      1. find all redundant FDs and put into fd_redundant, ie it can be inferred from other FDs so safe to remove\n",
    "      2. find all FDs that has to be present in the final result and put into fd_to_keep\n",
    "      3. find all subset of fd_redundant, as each of it can be a candidate in the final min cover\n",
    "      4. for each of the candidate subset sub, check whether sub + fd_to_keep is a min cover by\n",
    "         a. any of its FD cannot be removed\n",
    "         b. its all closures is the same as the original all closures\n",
    "      5. since each sub is unique, we can simply add sub + fd_to_keep to the final result if it pass the above criteria\n",
    "    Note: I am not using the algorithm mentioned in notes to consider all different orders and\n",
    "             try to remove redundant FDs one by one because it has a complexity of n!\n",
    "          That algorithm works find when we only want any one of the min covers(what I implemented in min_cover)\n",
    "          By checking all possible redundant subsets, the complexity is capped at 2^n, which is much better than n!.\n",
    "          When redundant FDs are not many it's even better.\n",
    "    '''\n",
    "    results = []\n",
    "    new_fds = min_covers_step1(R, FD)\n",
    "    full_closures = all_closures(R, FD)\n",
    "    for new_fd in new_fds:\n",
    "        fd_to_keep = []\n",
    "        fd_redundant = []\n",
    "        for i in range(len(new_fd)):\n",
    "            redundant = False\n",
    "            left = new_fd[i][0]\n",
    "            right = new_fd[i][1]\n",
    "            new_closures = all_closures(R, new_fd[:i] + new_fd[i + 1:])\n",
    "            for new_closure in new_closures:\n",
    "                if set_equals(new_closure[0], left):\n",
    "                    if is_subset(new_closure[1], right):\n",
    "                        redundant = True\n",
    "                    break\n",
    "                # this rule is not found in all closures,\n",
    "                # meaning it is a super key hence redundant\n",
    "                if len(new_closure[0]) > len(left):\n",
    "                    redundant = True\n",
    "                    break\n",
    "            if redundant:\n",
    "                fd_redundant.append(new_fd[i])\n",
    "            else:\n",
    "                fd_to_keep.append(new_fd[i])\n",
    "        fd_redundant_sub = []\n",
    "        # find all subsets of all redundant FDs\n",
    "        get_all_subset([], fd_redundant, fd_redundant_sub)\n",
    "        # for each subset, check whether it is a min cover\n",
    "        for sub in fd_redundant_sub:\n",
    "            temp_result = sub + fd_to_keep\n",
    "            is_minimal = True\n",
    "            # check whether all FDs in this set are not redundant\n",
    "            for i in range(len(temp_result)):\n",
    "                left = temp_result[i][0]\n",
    "                right = temp_result[i][1]\n",
    "                new_closures = all_closures(R, temp_result[:i] + temp_result[i + 1:])\n",
    "                for new_closure in new_closures:\n",
    "                    if set_equals(new_closure[0], left):\n",
    "                        if is_subset(new_closure[1], right):\n",
    "                            is_minimal = False\n",
    "                        break\n",
    "                    # this rule is not found in all closures,\n",
    "                    # meaning it is a super key hence redundant\n",
    "                    if len(new_closure[0]) > len(left):\n",
    "                        is_minimal = False\n",
    "                        break\n",
    "            # check whether all closures of this set is same as original\n",
    "            if is_minimal:\n",
    "                closures_to_check = all_closures(R, temp_result)\n",
    "                if fd_equals(closures_to_check, full_closures):\n",
    "                    results.append(temp_result)\n",
    "    return results\n",
    "\n",
    "\n",
    "## Return all minimal covers of a given schema R and functional dependencies F.\n",
    "def all_min_covers(R, FD):\n",
    "    '''\n",
    "    find all closures \"closures\" of the original set of FD\n",
    "    start from \"closures\" to find all of its min covers\n",
    "    '''\n",
    "    closures = all_closures(R, FD)\n",
    "    return min_covers(R, closures)\n",
    "\n",
    "\n",
    "## You can add additional functions below\n",
    "# helper function to check whether 2 sets(list) are equals\n",
    "def set_equals(set1, set2):\n",
    "    return len(set1) == len(set2) and is_subset(set1, set2)\n",
    "\n",
    "\n",
    "# helper function to check whether small sets(list) is a subset of big\n",
    "def is_subset(big, small):\n",
    "    for item in small:\n",
    "        if item not in big:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# recursive function to find all subset of certain length\n",
    "def get_all_set_rec(remaining_list, length, current_list, result_list, super_key_list):\n",
    "    # furthermore the subset should not be a supreset of any superkey\n",
    "    if len(super_key_list) > 0:\n",
    "        for super_key in super_key_list:\n",
    "            # if current_list is a subset of any superkey,\n",
    "            #  stop as per requirment\n",
    "            if is_subset(current_list, super_key):\n",
    "                return\n",
    "    # found a subset of required length, add it to result_list and return\n",
    "    if len(current_list) == length:\n",
    "        result_list.append(current_list)\n",
    "        return\n",
    "    # no more item to explore, return\n",
    "    if len(remaining_list) == 0:\n",
    "        return\n",
    "    item = remaining_list[0]\n",
    "    current_list_copy = current_list.copy()\n",
    "    current_list_copy.append(item)\n",
    "    remaining_list_copy = remaining_list.copy()\n",
    "    remaining_list_copy.remove(item)\n",
    "    # branch with 1st item in the remaining_list added to current_list\n",
    "    get_all_set_rec(remaining_list_copy, length, current_list_copy, result_list, super_key_list)\n",
    "    # branch with 1st item in the remaining_list NOT added to current_list\n",
    "    get_all_set_rec(remaining_list_copy, length, current_list.copy(), result_list, super_key_list)\n",
    "\n",
    "\n",
    "def min_cover_step1(R, FD):\n",
    "    new_fd = []\n",
    "    # simplify right hand side\n",
    "    for dependency in FD:\n",
    "        left = dependency[0]\n",
    "        right = dependency[1]\n",
    "        for item in right:\n",
    "            new_fd.append([left, [item]])\n",
    "    closures = all_closures(R, FD)\n",
    "    # remove trivial FD where RHS is a subset of LHS\n",
    "    new_fd = [item for item in new_fd if not is_subset(item[0], item[1])]\n",
    "    # for each FD, check whether there is a proper subset that can replace it\n",
    "    for fd_item in new_fd:\n",
    "        fd_left = fd_item[0]\n",
    "        for closure in closures:\n",
    "            closure_left = closure[0]\n",
    "            # if size is already greater or equal to LHS of this FD,\n",
    "            # no need to check further\n",
    "            # because the closures are ordered by the size of the set at LHS\n",
    "            if len(closure_left) >= len(fd_left):\n",
    "                break\n",
    "            # if RHS of this FD is a subset of RHS of a closure,\n",
    "            # and LHS of the closure is a proper subset of LHS of this FD\n",
    "            # simplify LHS of this FD by using LHS of the closure\n",
    "            if is_subset(closure[1], fd_item[1]) and is_subset(fd_left, closure_left):\n",
    "                fd_item[0] = closure_left\n",
    "                break\n",
    "    return new_fd\n",
    "\n",
    "\n",
    "def min_cover_step2(R, new_fd):\n",
    "    # flag whether to continue\n",
    "    to_continue = True\n",
    "    while to_continue:\n",
    "        # in each loop, init to_continue to False\n",
    "        to_continue = False\n",
    "        for i in range(len(new_fd)):\n",
    "            left = new_fd[i][0]\n",
    "            right = new_fd[i][1]\n",
    "            # calculte closures of the FD set by removing the ith FD\n",
    "            new_closures = all_closures(R, new_fd[:i] + new_fd[i + 1:])\n",
    "            for new_closure in new_closures:\n",
    "                # check whether this FD still exists in the new_closures\n",
    "                if set_equals(new_closure[0], left):\n",
    "                    if is_subset(new_closure[1], right):\n",
    "                        # if yest, then this FD is safe to be removed\n",
    "                        # remove it and set to_continue flag to True\n",
    "                        new_fd.pop(i)\n",
    "                        to_continue = True\n",
    "                    break\n",
    "                # this rule is not found in all closures,\n",
    "                # meaning it is a super key hence redundant\n",
    "                # remove it and set to_continue flag to True\n",
    "                if len(new_closure[0]) > len(left):\n",
    "                    new_fd.pop(i)\n",
    "                    to_continue = True\n",
    "                    break\n",
    "            # break the outer for loop and begin a new loop\n",
    "            # since new_fd has been modified\n",
    "            if to_continue:\n",
    "                break\n",
    "    return new_fd\n",
    "\n",
    "\n",
    "# helper function in finding min_covers, detailed explaination in min_cover comments\n",
    "def min_covers_step1(R, FD):\n",
    "    new_fd = []\n",
    "    # simplify right hand side\n",
    "    for dependency in FD:\n",
    "        left = dependency[0]\n",
    "        right = dependency[1]\n",
    "        for item in right:\n",
    "            new_fd.append([left, [item]])\n",
    "    closures = all_closures(R, FD)\n",
    "    # remove trivial FD where RHS is a subset of LHS\n",
    "    new_fd = [item for item in new_fd if not is_subset(item[0], item[1])]\n",
    "    choices = []\n",
    "    static_items = []\n",
    "    # find all possibilities to simplify LHS\n",
    "    for fd_item in new_fd:\n",
    "        choice = []\n",
    "        fd_left = fd_item[0]\n",
    "        for closure in closures:\n",
    "            closure_left = closure[0]\n",
    "            if len(closure_left) >= len(fd_item[0]):\n",
    "                break\n",
    "            # find a possible choice to replace the LHS\n",
    "            if is_subset(closure[1], fd_item[1]) and is_subset(fd_item[0], closure_left):\n",
    "                fd_left = closure_left\n",
    "                to_add_choice = True\n",
    "                # check whether this choice is already added\n",
    "                for existing_choice in choice:\n",
    "                    if is_subset(closure_left, existing_choice[0]):\n",
    "                        to_add_choice = False\n",
    "                        break\n",
    "                if to_add_choice:\n",
    "                    choice.append([closure_left, fd_item[1]])\n",
    "        # only add the found choice into choices list if\n",
    "        #  1. it has more than 1 items\n",
    "        #  2. it is not already in choices\n",
    "        if len(choice) > 1:\n",
    "            is_new_choice = True\n",
    "            for choice_item in choices:\n",
    "                if set_equals(choice_item, choice):\n",
    "                    is_new_choice = False\n",
    "                    break\n",
    "            if is_new_choice:\n",
    "                choices.append(choice)\n",
    "        # otherwise add the current item as it cannot be simplify or it only has 1 way to simplify\n",
    "        else:\n",
    "            static_items.append([fd_left, fd_item[1]])\n",
    "    all_combi = []\n",
    "    # get all possible comibations if any of the FD can be simplified in multiple ways\n",
    "    get_all_combi([], choices, all_combi)\n",
    "    result = []\n",
    "    # reomve all the duplicates and only keep unique results\n",
    "    for combi in all_combi:\n",
    "        temp_fd = remove_fd_dup(combi + static_items)\n",
    "        is_new = True\n",
    "        for existing in result:\n",
    "            if fd_equals(temp_fd, existing):\n",
    "                is_new = False\n",
    "                break\n",
    "        if is_new:\n",
    "            result.append(temp_fd)\n",
    "    return result\n",
    "\n",
    "\n",
    "# helper function to remove duplicates inside a set of FD\n",
    "def remove_fd_dup(fd):\n",
    "    result = []\n",
    "    for i in range(len(fd)):\n",
    "        found = False\n",
    "        for j in range(i + 1, len(fd)):\n",
    "            if fd_equals([fd[i]], [fd[j]]):\n",
    "                found = True\n",
    "                break\n",
    "        # only keep the fd if it's not found in the later part of this list\n",
    "        if not found:\n",
    "            result.append(fd[i])\n",
    "    return result\n",
    "\n",
    "\n",
    "# helper function to find all combinations of a list of items\n",
    "def get_all_combi(current_list, remain_list, result):\n",
    "    if len(remain_list) == 0:\n",
    "        result.append(current_list)\n",
    "        return\n",
    "    for i in range(len(remain_list[0])):\n",
    "        get_all_combi(current_list + [remain_list[0][i]], remain_list[1:], result)\n",
    "\n",
    "\n",
    "# helper function to find all subset of a set(list) of items\n",
    "def get_all_subset(current_list, remain_list, result):\n",
    "    if len(remain_list) == 0:\n",
    "        result.append(current_list)\n",
    "        return\n",
    "    get_all_subset(current_list + [remain_list[0]], remain_list[1:], result)\n",
    "    get_all_subset(current_list.copy(), remain_list[1:], result)\n",
    "\n",
    "\n",
    "# helper function to check whether 2 sets of FD are equal\n",
    "def fd_equals(fd1_list, fd2_list):\n",
    "    if len(fd1_list) != len(fd2_list):\n",
    "        return False\n",
    "    for fd1 in fd1_list:\n",
    "        found = False\n",
    "        left1 = fd1[0]\n",
    "        right1 = fd1[1]\n",
    "        for fd2 in fd2_list:\n",
    "            left2 = fd2[0]\n",
    "            right2 = fd2[1]\n",
    "            if set_equals(left1, left2) and set_equals(right1, right2):\n",
    "                found = True\n",
    "        if not found:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def find_all_candidate_keys(R, F):\n",
    "    all_subsets = get_all_subsets(R)\n",
    "    result = []\n",
    "\n",
    "    all_subsets_copy = all_subsets.copy()\n",
    "\n",
    "    while len(all_subsets_copy) > 0:\n",
    "        attr_set = all_subsets_copy.pop(0)\n",
    "        attr_set_closure = closure(R, F, list(attr_set))\n",
    "        if len(attr_set_closure) == 0:\n",
    "            continue\n",
    "        if set(attr_set_closure) == set(R):\n",
    "            all_subsets_copy = [attr_set_copy for attr_set_copy in all_subsets_copy if not set(attr_set_copy).issuperset(set(attr_set))]\n",
    "            result.append(list(attr_set))\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_prime_attributes(candidate_keys):\n",
    "    return list(set([k for ckey in candidate_keys for k in ckey]))\n",
    "\n",
    "def is_proper_subset(set1, set2):\n",
    "    return set1.issubset(set2) and not set2.issubset(set1)\n",
    "\n",
    "def is_3NF(R, F):\n",
    "    minimal_cover = min_cover(R, F)\n",
    "    print(\"Minimal cover: \", minimal_cover)\n",
    "    candidate_keys = find_all_candidate_keys(R, F)\n",
    "    print(\"Candidate keys: \", candidate_keys)\n",
    "    prime_attributes = get_prime_attributes(candidate_keys)\n",
    "    print(\"Prime attributes: \", prime_attributes, end=\"\\n\")\n",
    "    violating_fds = []\n",
    "    is_3nf = True\n",
    "    for fd in minimal_cover:\n",
    "        lhs = fd[0]\n",
    "        rhs = fd[1][0]\n",
    "        for ckey in candidate_keys:\n",
    "            if not set(lhs).issuperset(set(ckey)):\n",
    "                if rhs not in prime_attributes:\n",
    "                    print(\"-\"*30)\n",
    "                    print(\"Current fd: \", fd)\n",
    "                    print(\"Current candidate key: \", ckey)\n",
    "                    print(f\"{lhs} is not a super key\")\n",
    "                    print(f\"{rhs} is not a prime attribute\")\n",
    "                    if fd not in violating_fds:\n",
    "                        violating_fds.append(fd)\n",
    "                    is_3nf = False\n",
    "    \n",
    "    return is_3nf, violating_fds\n",
    "\n",
    "def compact_min_cover(minimal_cover):\n",
    "    lhs_map = {}\n",
    "    for fd in minimal_cover:\n",
    "        lhs, rhs = fd[0], fd[1]\n",
    "        hashable_lhs = tuple(sorted(lhs))\n",
    "        if hashable_lhs not in lhs_map:\n",
    "            lhs_map[hashable_lhs] = set(rhs)\n",
    "        else:\n",
    "            lhs_map[hashable_lhs].update(set(rhs))\n",
    "    \n",
    "    return [[list(lhs), list(rhs)] for lhs, rhs in lhs_map.items()]\n",
    "    \n",
    "def is_BCNF(R, F, verbose=True):\n",
    "    minimal_cover = min_cover(R, F)\n",
    "    minimal_cover = compact_min_cover(minimal_cover)\n",
    "    candidate_keys = find_all_candidate_keys(R, F)\n",
    "    if verbose:\n",
    "        print(f\"Checking Relation {R} with FD set {F}\")\n",
    "        print(\"Minimal cover: \", minimal_cover)\n",
    "        print(\"Candidate keys: \", candidate_keys)\n",
    "    is_bcnf = True\n",
    "    violating_fds = []\n",
    "    # Check if any fd's lhs is not a superkey\n",
    "    for fd in minimal_cover:\n",
    "        lhs = fd[0]\n",
    "        # Check if current fd's lhs is a superkey by checking if it is a subset of any candidate_keys\n",
    "        is_lhs_superkey = False\n",
    "        for ckey in candidate_keys:\n",
    "            if set(lhs).issuperset(set(ckey)):\n",
    "                is_lhs_superkey = True\n",
    "        # If not a superkey, then BCNF is violated. Add fd to violating_fds if not exists\n",
    "        if not is_lhs_superkey:\n",
    "            if fd not in violating_fds:\n",
    "                violating_fds.append(fd)\n",
    "            is_bcnf = False\n",
    "    \n",
    "    return is_bcnf, violating_fds\n",
    "\n",
    "def is_2NF(R, F):\n",
    "    minimal_cover = min_cover(R, F)\n",
    "    print(\"Minimal cover: \", minimal_cover)\n",
    "    candidate_keys = find_all_candidate_keys(R, F)\n",
    "    print(\"Candidate keys: \", candidate_keys)\n",
    "    prime_attributes = get_prime_attributes(candidate_keys)\n",
    "    print(\"Prime attributes: \", prime_attributes, end=\"\\n\")\n",
    "    violating_fds = []\n",
    "    is_2nf = True\n",
    "    for fd in minimal_cover:\n",
    "        lhs = fd[0]\n",
    "        rhs = fd[1][0]\n",
    "        for ckey in candidate_keys:\n",
    "            if is_proper_subset(set(lhs), set(ckey)):\n",
    "                if rhs not in prime_attributes:\n",
    "                    print(\"-\"*30)\n",
    "                    print(\"Current fd: \", fd)\n",
    "                    print(f\"{lhs} is a proper subset of candidate key {ckey}\")\n",
    "                    print(f\"{rhs} is not a prime attribute\")\n",
    "                    if fd not in violating_fds:\n",
    "                        violating_fds.append(fd)\n",
    "                    is_2nf = False\n",
    "    \n",
    "    return is_2nf, violating_fds\n",
    "\n",
    "def project(parent_R, decomposed_R, parent_F):\n",
    "    decomposed_F = []\n",
    "    for attr_set in chain.from_iterable(combinations(decomposed_R, r) for r in range(1, len(decomposed_R) + 1)):\n",
    "        attr_set_closure = closure(decomposed_R, parent_F, list(attr_set))\n",
    "\n",
    "        for attr in attr_set_closure:\n",
    "            if attr in decomposed_R:\n",
    "                decomposed_F.append([list(attr_set), list(attr)])\n",
    "    \n",
    "    minimal_cover = min_cover(decomposed_R, decomposed_F)\n",
    "    return minimal_cover\n",
    "\n",
    "def decomposision(R, F, accum):\n",
    "    is_bcnf, violating_fds = is_BCNF(R, F, verbose=True)\n",
    "    if not is_bcnf:\n",
    "        violating_fd = violating_fds[0]\n",
    "        lhs = violating_fd[0]\n",
    "        R1 = closure(R, F, lhs)\n",
    "        R2 = list(set(R).difference(set(R1)).union(set(lhs)))\n",
    "        F1 = project(R, R1, F)\n",
    "        F2 = project(R, R2, F)\n",
    "        print(f\"Violating Relation: {R}\")\n",
    "        print(f\"Violating fds: {violating_fds}\")\n",
    "        print(f\"R1: {R1} - F1: {F1}\")\n",
    "        print(f\"R2: {R2} - F2: {F2}\")\n",
    "        print(\"-\" * 30)\n",
    "        decomposision(R1, F1, accum)\n",
    "        decomposision(R2, F2, accum)\n",
    "    else:\n",
    "        accum.append((R, F))\n",
    "    \n",
    "    return accum\n",
    "\n",
    "def remove_subsumed_fragments(lst):\n",
    "    curr_res = []\n",
    "    result = []\n",
    "    # map each element to set and sort by decreasing size\n",
    "    for fragment in sorted(map(set, lst), key = len, reverse = True):\n",
    "        # if current fragment is not a subset of any fragment in curr_res, add it to curr_res. This helps to remove duplicate fragment of same size also as the next occurrence will be a subset hence not added.\n",
    "        if not any(fragment.issubset(req) for req in curr_res):\n",
    "            curr_res.append(fragment)\n",
    "            result.append(list(fragment))\n",
    "          \n",
    "    return result\n",
    "\n",
    "def make_fragments(compact_minimal_cover, candidate_keys):\n",
    "    fragments = [lhs + rhs for lhs, rhs in compact_minimal_cover]\n",
    "    print(f\"Original Fragments {fragments}\")\n",
    "    # remove subsumed fragments (any fragment which is a proper subset of any other fragment)\n",
    "    no_subsumed_fragments = remove_subsumed_fragments(fragments)\n",
    "    print(f\"After remove subsumed fragments {no_subsumed_fragments}\")\n",
    "    \n",
    "    # check if any fragments contains a candidate key\n",
    "    has_ckey = False\n",
    "    for fragment in no_subsumed_fragments:\n",
    "        for ckey in candidate_keys:\n",
    "            if set(ckey).issubset(set(fragment)):\n",
    "                has_ckey = True\n",
    "    print(f\"Current fragments have candidate key? {has_ckey}\")\n",
    "    if not has_ckey:\n",
    "        no_subsumed_fragments.append(candidate_keys[0])\n",
    "    print(f\"Append {candidate_keys[0]} since no fragment contains a candidate key\")\n",
    "\n",
    "    return no_subsumed_fragments\n",
    "\n",
    "def synthesis(R, F):\n",
    "    print(f\"Relation {R} with FD set {F}\")\n",
    "    candidate_keys = find_all_candidate_keys(R, F)\n",
    "    print(f\"Candidate keys {candidate_keys}\")\n",
    "    minimal_cover = min_cover(R, F)\n",
    "    compact_minimal_cover = compact_min_cover(minimal_cover)\n",
    "    print(f\"Compact min cover: {compact_minimal_cover}\")\n",
    "    fragments = make_fragments(compact_minimal_cover, candidate_keys)\n",
    "    print(f\"Fragments {fragments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Relation ['A', 'B', 'C', 'D', 'E'] with FD set [[['A'], ['A', 'B', 'C']], [['A', 'B'], ['A']], [['B', 'C'], ['A', 'D']], [['B'], ['A', 'B']], [['C'], ['D']]]\n",
      "Minimal cover:  [[['A'], ['B', 'C']], [['B'], ['A']], [['C'], ['D']]]\n",
      "Candidate keys:  [['A', 'E'], ['B', 'E']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, [[['A'], ['B', 'C']], [['B'], ['A']], [['C'], ['D']]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R = ['A', 'B', 'C', 'D', 'E']\n",
    "# F = [[['A', 'B'], ['C', 'D', 'E']], [['A', 'C'], ['B', 'D', 'E']], [['B'], ['C']], [['C'], ['B']], [['C'], ['D']], [['B'], ['E']], [['C'], ['E']]]\n",
    "\n",
    "R = ['A', 'B', 'C', 'D', 'E']\n",
    "F = [[['A'], ['A', 'B', 'C']], [['A', 'B'], ['A']], [['B', 'C'], ['A', 'D']], [['B'], ['A', 'B']], [['C'], ['D']]]\n",
    "\n",
    "# min_cover(R, F)\n",
    "# is_2NF(R, F)\n",
    "# is_3NF(R, F)\n",
    "is_BCNF(R, F, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Relation ['A', 'B', 'C', 'D', 'E'] with FD set [[['A'], ['A', 'B', 'C']], [['A', 'B'], ['A']], [['B', 'C'], ['A', 'D']], [['B'], ['A', 'B']], [['C'], ['D']]]\n",
      "Minimal cover:  [[['A'], ['B', 'C']], [['B'], ['A']], [['C'], ['D']]]\n",
      "Candidate keys:  [['A', 'E'], ['B', 'E']]\n",
      "Violating Relation: ['A', 'B', 'C', 'D', 'E']\n",
      "Violating fds: [[['A'], ['B', 'C']], [['B'], ['A']], [['C'], ['D']]]\n",
      "R1: ['A', 'B', 'C', 'D'] - F1: [[['C'], ['D']], [['A'], ['C']], [['A'], ['B']], [['B'], ['A']]]\n",
      "R2: ['E', 'A'] - F2: []\n",
      "------------------------------\n",
      "Checking Relation ['A', 'B', 'C', 'D'] with FD set [[['C'], ['D']], [['A'], ['C']], [['A'], ['B']], [['B'], ['A']]]\n",
      "Minimal cover:  [[['C'], ['D']], [['A'], ['B', 'C']], [['B'], ['A']]]\n",
      "Candidate keys:  [['A'], ['B']]\n",
      "Violating Relation: ['A', 'B', 'C', 'D']\n",
      "Violating fds: [[['C'], ['D']]]\n",
      "R1: ['C', 'D'] - F1: [[['C'], ['D']]]\n",
      "R2: ['B', 'A', 'C'] - F2: [[['B'], ['C']], [['B'], ['A']], [['A'], ['B']]]\n",
      "------------------------------\n",
      "Checking Relation ['C', 'D'] with FD set [[['C'], ['D']]]\n",
      "Minimal cover:  [[['C'], ['D']]]\n",
      "Candidate keys:  [['C']]\n",
      "Checking Relation ['B', 'A', 'C'] with FD set [[['B'], ['C']], [['B'], ['A']], [['A'], ['B']]]\n",
      "Minimal cover:  [[['B'], ['A', 'C']], [['A'], ['B']]]\n",
      "Candidate keys:  [['B'], ['A']]\n",
      "Checking Relation ['E', 'A'] with FD set []\n",
      "Minimal cover:  []\n",
      "Candidate keys:  [['E', 'A']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['C', 'D'], [[['C'], ['D']]]),\n",
       " (['B', 'A', 'C'], [[['B'], ['C']], [['B'], ['A']], [['A'], ['B']]]),\n",
       " (['E', 'A'], [])]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decomposision(R, F, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation ['A', 'B', 'C', 'D', 'E'] with FD set [[['A'], ['A', 'B', 'C']], [['A', 'B'], ['A']], [['B', 'C'], ['A', 'D']], [['B'], ['A', 'B']], [['C'], ['D']]]\n",
      "Candidate keys [['A', 'E'], ['B', 'E']]\n",
      "Compact min cover: [[['A'], ['B', 'C']], [['B'], ['A']], [['C'], ['D']]]\n",
      "Original Fragments [['A', 'B', 'C'], ['B', 'A'], ['C', 'D']]\n",
      "After remove subsumed fragments [['B', 'A', 'C'], ['D', 'C']]\n",
      "Current fragments have candidate key? False\n",
      "Append ['A', 'E'] since no fragment contains a candidate key\n",
      "Fragments [['B', 'A', 'C'], ['D', 'C'], ['A', 'E']]\n"
     ]
    }
   ],
   "source": [
    "synthesis(R, F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d97dbaaf0f8d39e846d8bff06dfa96e0d39161eb68e322a2f5c8dc0058611f2b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('covidRL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
